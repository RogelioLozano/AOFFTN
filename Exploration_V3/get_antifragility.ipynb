{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def nrml(a):    # Normalize in [0,1]\n",
    "    return (a - a.min()) / (a.max() - a.min())\n",
    "\n",
    "def nrml_(a):   # Normalize in [-1,1]\n",
    "    return 2*(a - a.min()) / (a.max() - a.min())-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pairing_data/for_distance_pairing/trade_matrix_final.csv\")\n",
    "df = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop=pd.read_csv('pairing_data/Pop_year_ex_im.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE Antifragility normalized cases\n",
    "\n",
    "# calories data\n",
    "\n",
    "dict_100g_kcal = {\n",
    "'Maize': 356,\n",
    "'Chick peas' : 358,\n",
    "'Rice - total  (Rice milled equivalent)' : 360,\n",
    "'Wheat': 334,\n",
    "'Beans, dry': 341,\n",
    "'Groundnuts, shelled' : 567,\n",
    "'Soybeans': 335,\n",
    "'Oats': 385,\n",
    "'Barley': 332,\n",
    "'Millet': 340,\n",
    "'Sorghum': 343\n",
    "}\n",
    "\n",
    "caloric=df.copy(deep=True)\n",
    "caloric['caloric']=caloric['Item'].map(dict_100g_kcal)\n",
    "caloric['Value']=caloric['Value']*nrml(caloric['caloric'])\n",
    "df_caloric=caloric.drop(columns=['caloric'])\n",
    "\n",
    "# distance data\n",
    "\n",
    "distances_YEI=pd.read_csv('pairing_data/for_distance_pairing/distances_year_ex_im.csv',index_col=0)\n",
    "distance=distances_YEI.merge(df,on=['exporter','importer','Year'],how='inner')\n",
    "distance['Value']=distance['Value']*nrml(distance['distance'])\n",
    "df_distance=distance.drop(columns=['distance'])\n",
    "\n",
    "# # population data\n",
    "\n",
    "pop=pd.read_csv('pairing_data/Pop_year_ex_im.csv',index_col=0)\n",
    "pop=df.merge(pop,how='inner',left_on=['importer','exporter','Year'],right_on=['importer','exporter','date']).drop(['date'],axis=1)\n",
    "pop['Value']=pop['Value']*nrml(pop['avg_pop'])\n",
    "df_pop=pop.drop(columns=['avg_pop'])\n",
    "\n",
    "# # gdp data\n",
    "\n",
    "gdp=pd.read_csv('pairing_data/GDP_year_ex_im.csv',index_col=0)\n",
    "gdp=df.merge(gdp,how='inner',left_on=['importer','exporter','Year'],right_on=['importer','exporter','date']).drop(['date'],axis=1)\n",
    "gdp['Value'] = gdp['Value']*nrml(gdp['avg_gdp'])\n",
    "df_gdp=gdp.drop(columns=['avg_gdp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antifragility(trade_data):\n",
    "    \n",
    "    df=trade_data\n",
    "    \n",
    "    # Generate NETWORK\n",
    "    lg = []    #  lg = list of graphs:  a network (graph) for each year\n",
    "    for year in range(1986, 2018):\n",
    "        G = nx.MultiDiGraph(weighted=True)\n",
    "        for index, row in df.loc[df['Year']==year].iterrows():\n",
    "            if row['Element']=='Import Quantity':\n",
    "                G.add_edge( row['exporter'], row['importer'], row['Item'], weight=row['Value'],   )\n",
    "            else:\n",
    "                G[row['exporter']][row['importer']][row['Item']]['value'] = row['Value']\n",
    "        lg += [G]\n",
    "    \n",
    "    # create a dictionary with global amount of trade\n",
    "    # quantity = Total imports \n",
    "    # mean_val = Average price \n",
    "\n",
    "    items = df.groupby('Item').groups.keys()\n",
    "    glob_dict = {}            \n",
    "    py = df.groupby('Year')  # py = Per Year \n",
    "    for year, group in py:\n",
    "        glob_dict[year] = {}\n",
    "        for itm in items:\n",
    "            glob_dict[year][itm] = {}\n",
    "            glob_dict[year][itm]['quantity'] = group.loc[group['Item']==itm].loc[group['Element']=='Import Quantity']['Value'].sum()\n",
    "            glob_dict[year][itm]['mean_val'] = group.loc[group['Item']==itm].loc[group['Element']=='Import Value']['Value'].mean()\n",
    "\n",
    "    norm_dict = {}    # create a new dictionary for normalizing values\n",
    "    for itm in items:\n",
    "        norm_dict[itm] = {}\n",
    "        for year in range(1986, 2018):\n",
    "            norm_dict[itm][year] = {}\n",
    "            norm_dict[itm][year]['qt'] = glob_dict[year][itm]['quantity']\n",
    "            norm_dict[itm][year]['vl'] = glob_dict[year][itm]['mean_val']\n",
    "\n",
    "    for itm in items:  # add normalized values to global yearly dictionary \n",
    "        dfa = pd.DataFrame(norm_dict[itm]).transpose()\n",
    "        dfa['nqt'] = nrml(dfa['qt'])\n",
    "        dfa['nvl'] = nrml(dfa['vl'])\n",
    "        for year in range(1986, 2018):\n",
    "            glob_dict[year][itm]['qnt'] = dfa.loc[year]['nqt']\n",
    "            glob_dict[year][itm]['val'] = dfa.loc[year]['nvl']\n",
    "\n",
    "    for year in range(1987, 2018):  #  Calculate differences of trade values on consecutive years \n",
    "        for itm in items:\n",
    "            glob_dict[year][itm]['delta_pr'] = np.abs(glob_dict[year][itm]['val'] - glob_dict[year-1][itm]['val'])\n",
    "            glob_dict[year][itm]['delta_qt'] = np.abs(glob_dict[year][itm]['qnt'] - glob_dict[year-1][itm]['qnt'])\n",
    "\n",
    "    norm_dict2 = {}    # create a dictionary for normalizing delta values\n",
    "    for itm in items:\n",
    "        norm_dict2[itm] = {}\n",
    "        for year in range(1987, 2018):\n",
    "            norm_dict2[itm][year] = {}\n",
    "            norm_dict2[itm][year]['delta_pr'] = glob_dict[year][itm]['delta_pr']\n",
    "            norm_dict2[itm][year]['delta_qt'] = glob_dict[year][itm]['delta_qt']\n",
    "\n",
    "\n",
    "    for itm in items:  # add normalized delta values to global yearly dictionary \n",
    "        dfa = pd.DataFrame(norm_dict2[itm]).transpose()\n",
    "        dfa['delta_pr_n'] = nrml(dfa['delta_pr'])\n",
    "        dfa['delta_qt_n'] = nrml(dfa['delta_qt'])\n",
    "        for year in range(1987, 2018):\n",
    "            glob_dict[year][itm]['pertb_pr'] = dfa.loc[year]['delta_pr_n']\n",
    "            glob_dict[year][itm]['pertb_qt'] = dfa.loc[year]['delta_qt_n']\n",
    "\n",
    "    for year in range(1987, 2018):  # take yearly mean of perturbation over Item list ...\n",
    "        glob_dict[year]['PERTB_pr'] = pd.DataFrame(glob_dict[year]).transpose()['pertb_pr'].mean()\n",
    "        glob_dict[year]['PERTB_qt'] = pd.DataFrame(glob_dict[year]).transpose()['pertb_qt'].mean()\n",
    "\n",
    "    DF = pd.DataFrame(glob_dict).transpose()\n",
    "    DF['perturb_pr'] = nrml(DF['PERTB_pr'])  # ... and Normalize them \n",
    "    DF['perturb_qt'] = nrml(DF['PERTB_qt'])\n",
    "    for year in range(1987, 2018):             # Add normalized perturbations values to global dictionary \n",
    "        glob_dict[year]['pertN_pr'] = DF.loc[year]['perturb_pr']\n",
    "        glob_dict[year]['pertN_qt'] = DF.loc[year]['perturb_qt']\n",
    "\n",
    "\n",
    "    dict_perts = {}    #   Create dictionary and DataFrame for perturbations \n",
    "    for year in range(1987, 2018):\n",
    "        dict_perts[year] = {}\n",
    "        for x in ['pertN_pr','pertN_qt']:\n",
    "            dict_perts[year][x] = glob_dict[year][x]\n",
    "        for itm in items:\n",
    "            for y in ['pertb_pr', 'pertb_qt']:\n",
    "                dict_perts[year][itm+y[-3:]] = glob_dict[year][itm][y]\n",
    "\n",
    "\n",
    "    dfperts = pd.DataFrame(dict_perts).transpose()    \n",
    "\n",
    "\n",
    "\n",
    "    #  create a dictionary with amount of trade per country\n",
    "    # quantity = Total imports \n",
    "    # mean_val = Average price     \n",
    "\n",
    "    items = df.groupby('Item').groups.keys()\n",
    "    dict_country = {}            \n",
    "    pc = df.groupby('Year')  # pc = Per Country \n",
    "\n",
    "    for name, group in df.groupby('importer'):\n",
    "        dict_country[name] = {}\n",
    "\n",
    "    for name, group in df.groupby('exporter'):\n",
    "        if name not in dict_country.keys():\n",
    "            dict_country[name] = {}\n",
    "\n",
    "    year = int(1986)\n",
    "    for G in lg:\n",
    "        for x in G.nodes():\n",
    "            try:\n",
    "                dict_country[x][year] = {}\n",
    "                for y in G.out_edges(x):        \n",
    "                    for key, val in G[y[0]][y[1]].items():\n",
    "                        for k, v in val.items():\n",
    "                            dict_country[x][year]['exp_'+key+'_'+k] = v\n",
    "                for y in G.in_edges(x):\n",
    "                    for key, val in G[y[0]][y[1]].items():\n",
    "                        for k, v in val.items():\n",
    "                            dict_country[x][year]['imp_'+key+'_'+k] = v\n",
    "            except KeyError as e:\n",
    "                continue\n",
    "        year += 1\n",
    "\n",
    "\n",
    "    for key, val in dict_country.items(): # compute satisfaction measures\n",
    "        xxx = pd.DataFrame(val).transpose()\n",
    "        for col in xxx.columns:\n",
    "            xxx[col] = nrml(xxx[col])\n",
    "            xxx['dlt_'+col] = xxx[col] - xxx[col].shift(1)\n",
    "            xxx['dlt_'+col] = nrml_(xxx['dlt_'+col])\n",
    "        for year in range(1987, 2018):\n",
    "            try:\n",
    "                dict_country[key][year]['dlt_in_pr'] = xxx.drop([xc for xc in xxx.columns if 'dlt_imp_' != xc[:8] or '_value' not in xc ], axis=1).transpose()[year].mean()\n",
    "                dict_country[key][year]['dlt_in_vl'] = xxx.drop([xc for xc in xxx.columns if 'dlt_imp_' != xc[:8] or '_weight' not in xc ], axis=1).transpose()[year].mean()\n",
    "                dict_country[key][year]['dlt_ex_pr'] = xxx.drop([xc for xc in xxx.columns if 'dlt_exp_' != xc[:8] or '_value' not in xc ], axis=1).transpose()[year].mean()\n",
    "                dict_country[key][year]['dlt_ex_vl'] = xxx.drop([xc for xc in xxx.columns if 'dlt_exp_' != xc[:8] or '_weight' not in xc ], axis=1).transpose()[year].mean()\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "    dictotal = {}  # compute antifragility \n",
    "    i = 0\n",
    "    j = 0\n",
    "    lp = []\n",
    "    for country in dict_country.keys():\n",
    "        dictotal[country] = {}\n",
    "        for year in dict_perts.keys():\n",
    "            dictotal[country][year] = {}\n",
    "            try:\n",
    "                dictotal[country][year]['af_in_pr'] = -1*dict_country[country][year]['dlt_in_pr'] * dict_perts[year]['pertN_pr']\n",
    "                dictotal[country][year]['af_in_vl'] = -1*dict_country[country][year]['dlt_in_vl'] * dict_perts[year]['pertN_qt']\n",
    "                dictotal[country][year]['af_ex_pr'] = dict_country[country][year]['dlt_ex_pr'] * dict_perts[year]['pertN_pr']\n",
    "                dictotal[country][year]['af_ex_vl'] = dict_country[country][year]['dlt_ex_vl'] * dict_perts[year]['pertN_qt']\n",
    "                dictotal[country][year]['af_in_pr2'] = -1*dict_country[country][year]['dlt_in_pr'] * (dict_perts[year]['pertN_pr'] + dict_perts[year]['pertN_qt'])/2\n",
    "                dictotal[country][year]['af_in_vl2'] = -1*dict_country[country][year]['dlt_in_vl'] * (dict_perts[year]['pertN_pr'] + dict_perts[year]['pertN_qt'])/2\n",
    "                dictotal[country][year]['af_ex_pr2'] = dict_country[country][year]['dlt_ex_pr'] * (dict_perts[year]['pertN_pr'] + dict_perts[year]['pertN_qt'])/2\n",
    "                dictotal[country][year]['af_ex_vl2'] = dict_country[country][year]['dlt_ex_vl'] * (dict_perts[year]['pertN_pr'] + dict_perts[year]['pertN_qt'])/2\n",
    "            except:\n",
    "                j += 1\n",
    "                lp += [country]\n",
    "            i += 1\n",
    "    print('total loops = ',i, 'total exceptions = ',j)\n",
    "\n",
    "    reform = {(outerKey, innerKey): values for outerKey, innerDict in dictotal.items() for innerKey, values in innerDict.items()}\n",
    "    a = pd.DataFrame(reform).transpose()\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loops =  7533 total exceptions =  1926\n"
     ]
    }
   ],
   "source": [
    "a=antifragility(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loops =  7533 total exceptions =  1926\n"
     ]
    }
   ],
   "source": [
    "a_caloric=antifragility(df_caloric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loops =  6479 total exceptions =  1163\n"
     ]
    }
   ],
   "source": [
    "a_gdp = antifragility(df_gdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loops =  6479 total exceptions =  1163\n"
     ]
    }
   ],
   "source": [
    "a_pop=antifragility(df_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loops =  7533 total exceptions =  1926\n"
     ]
    }
   ],
   "source": [
    "a_distance=antifragility(df_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "afs={'af':a,\n",
    "     'afpop':a_pop,\n",
    "     'afgdp':a_gdp,\n",
    "     'afcaloric':a_caloric,\n",
    "     'afdistance':a_distance}\n",
    "\n",
    "directory='antifragilidades_datasets'\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "for name,antif in afs.items():\n",
    "    antif.to_csv(directory+f'/{name}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopandas_env",
   "language": "python",
   "name": "geopandas_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
